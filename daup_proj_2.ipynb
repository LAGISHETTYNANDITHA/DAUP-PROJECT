{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "rgb 256"
      ],
      "metadata": {
        "id": "exA02CDpSpmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define dataset paths\n",
        "train_dir = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/train\"\n",
        "val_dir = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/valid\"\n",
        "\n",
        "# Define label mapping\n",
        "label_mapping = {\"FRESH\": 0, \"SPOILED\": 1}\n",
        "\n",
        "# Function to extract label from filename\n",
        "def get_label_from_filename(filename):\n",
        "    if \"FRESH\" in filename.upper():\n",
        "        return label_mapping[\"FRESH\"]\n",
        "    elif \"SPOILED\" in filename.upper():\n",
        "        return label_mapping[\"SPOILED\"]\n",
        "    else:\n",
        "        return None  # Ignore unexpected filenames\n",
        "\n",
        "# Function to load images and labels\n",
        "def load_images(folder, img_size=(256, 256)):\n",
        "    images, labels = [], []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, file)\n",
        "\n",
        "        label = get_label_from_filename(file)\n",
        "        if label is not None:\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "                img = cv2.resize(img, img_size)  # Resize image\n",
        "                img = img / 255.0  # Normalize\n",
        "\n",
        "                images.append(img)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load train and validation data\n",
        "X_train, y_train = load_images(train_dir, img_size=(256, 256))\n",
        "X_val, y_val = load_images(val_dir, img_size=(256, 256))\n",
        "\n",
        "print(f\"Loaded {len(X_train)} training images and {len(X_val)} validation images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzBpifbPk4ks",
        "outputId": "9d255381-e76a-4992-9905-a8364876db3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1815 training images and 451 validation images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CNN Model with 3 Convolutional Layers\n",
        "def build_cnn(input_shape):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')  # Binary Classification (Fresh/Spoiled)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = build_cnn((256, 256, 3))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the CNN...\")\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {test_acc * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_B8vU2yr9No",
        "outputId": "1b0eaf10-3844-48b7-ff43-bb4098b19784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the CNN...\n",
            "Epoch 1/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 6s/step - accuracy: 0.6991 - loss: 0.9792 - val_accuracy: 0.7472 - val_loss: 0.5238\n",
            "Epoch 2/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 6s/step - accuracy: 0.8303 - loss: 0.4170 - val_accuracy: 0.7517 - val_loss: 0.4386\n",
            "Epoch 3/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 6s/step - accuracy: 0.8464 - loss: 0.3584 - val_accuracy: 0.8381 - val_loss: 0.3809\n",
            "Epoch 4/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 6s/step - accuracy: 0.8985 - loss: 0.2500 - val_accuracy: 0.8736 - val_loss: 0.2766\n",
            "Epoch 5/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 6s/step - accuracy: 0.9476 - loss: 0.1398 - val_accuracy: 0.9424 - val_loss: 0.1420\n",
            "Epoch 6/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 6s/step - accuracy: 0.9719 - loss: 0.0740 - val_accuracy: 0.9446 - val_loss: 0.1605\n",
            "Epoch 7/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 6s/step - accuracy: 0.9771 - loss: 0.0696 - val_accuracy: 0.9712 - val_loss: 0.0890\n",
            "Epoch 8/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 6s/step - accuracy: 0.9882 - loss: 0.0363 - val_accuracy: 0.9690 - val_loss: 0.0889\n",
            "Epoch 9/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 6s/step - accuracy: 0.9927 - loss: 0.0243 - val_accuracy: 0.9202 - val_loss: 0.2515\n",
            "Epoch 10/10\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 6s/step - accuracy: 0.9788 - loss: 0.0431 - val_accuracy: 0.9756 - val_loss: 0.1011\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.9906 - loss: 0.0413\n",
            "Validation Accuracy: 97.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Function to load and preprocess image from a URL or local path\n",
        "def preprocess_image(path, target_size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses an image from a URL or local file path.\n",
        "\n",
        "    Args:\n",
        "        path (str): The URL or local file path of the image.\n",
        "        target_size (tuple): The desired size of the image (width, height).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The preprocessed image as a NumPy array.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt to open the path as a URL\n",
        "        response = requests.get(path)\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    except (requests.exceptions.MissingSchema, requests.exceptions.InvalidSchema):\n",
        "        # If it's not a valid URL, assume it's a local file path\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "    img = img.resize(target_size)  # Resize image\n",
        "    img_array = np.array(img) / 255.0  # Normalize\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img_array\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_image(path, model):\n",
        "    img_array = preprocess_image(path)\n",
        "    prediction = model.predict(img_array)[0][0]  # Get single prediction\n",
        "    if prediction > 0.5:\n",
        "        print(\"Prediction: ❌ Spoiled Meat\")\n",
        "    else:\n",
        "        print(\"Prediction: ✅ Fresh Meat\")\n",
        "\n",
        "# Example usage\n",
        "image_url = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/train/FRESH-1-_JPG.rf.65663691924ca0aede3884b863267c98.jpg\"  # Replace with actual image URL or local file path\n",
        "predict_image(image_url, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngCnFYFz44xA",
        "outputId": "9ddb00b9-8cdf-4b64-a036-0b61d26ee7b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497ms/step\n",
            "Prediction: ✅ Fresh Meat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "greyscale 256"
      ],
      "metadata": {
        "id": "woSgpq9zSx1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define dataset paths\n",
        "train_dir = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/train\"\n",
        "val_dir = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/valid\"\n",
        "\n",
        "# Define label mapping\n",
        "label_mapping = {\"FRESH\": 0, \"SPOILED\": 1}\n",
        "\n",
        "# Function to extract label from filename\n",
        "def get_label_from_filename(filename):\n",
        "    if \"FRESH\" in filename.upper():\n",
        "        return label_mapping[\"FRESH\"]\n",
        "    elif \"SPOILED\" in filename.upper():\n",
        "        return label_mapping[\"SPOILED\"]\n",
        "    else:\n",
        "        return None  # Ignore unexpected filenames\n",
        "\n",
        "# Function to load images and labels (convert to grayscale)\n",
        "def load_images(folder, img_size=(256, 256)):\n",
        "    images, labels = [], []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, file)\n",
        "\n",
        "        label = get_label_from_filename(file)\n",
        "        if label is not None:\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, img_size)  # Resize image\n",
        "                img = img / 255.0  # Normalize\n",
        "\n",
        "                images.append(img)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(images).reshape(-1, img_size[0], img_size[1], 1), np.array(labels)  # Add channel dimension\n",
        "\n",
        "# Load train and validation data\n",
        "X_train, y_train = load_images(train_dir, img_size=(256, 256))\n",
        "X_val, y_val = load_images(val_dir, img_size=(256, 256))\n",
        "\n",
        "print(f\"Loaded {len(X_train)} training images and {len(X_val)} validation images\")\n"
      ],
      "metadata": {
        "id": "1yy3jZjaQtqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CNN Model with 3 Convolutional Layers\n",
        "def build_cnn(input_shape):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')  # Binary Classification (Fresh/Spoiled)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = build_cnn((256, 256, 3))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the CNN...\")\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {test_acc * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "7uTcsdOPQ4i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Function to load and preprocess image from a URL or local path (grayscale)\n",
        "def preprocess_image(path, target_size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses an image from a URL or local file path in grayscale.\n",
        "\n",
        "    Args:\n",
        "        path (str): The URL or local file path of the image.\n",
        "        target_size (tuple): The desired size of the image (width, height).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The preprocessed image as a NumPy array.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt to open the path as a URL\n",
        "        response = requests.get(path)\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"L\")  # Convert to grayscale\n",
        "    except (requests.exceptions.MissingSchema, requests.exceptions.InvalidSchema):\n",
        "        # If it's not a valid URL, assume it's a local file path\n",
        "        img = Image.open(path).convert(\"L\")  # Convert to grayscale\n",
        "\n",
        "    img = img.resize(target_size)  # Resize image\n",
        "    img_array = np.array(img) / 255.0  # Normalize\n",
        "    img_array = np.expand_dims(img_array, axis=(0, -1))  # Add batch and channel dimensions (for CNN input)\n",
        "\n",
        "    return img_array\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_image(path, model):\n",
        "    img_array = preprocess_image(path)\n",
        "    prediction = model.predict(img_array)[0][0]  # Get single prediction\n",
        "    if prediction > 0.5:\n",
        "        print(\"Prediction: ❌ Spoiled Meat\")\n",
        "    else:\n",
        "        print(\"Prediction: ✅ Fresh Meat\")\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/train/FRESH-1-_JPG.rf.65663691924ca0aede3884b863267c98.jpg\"  # Replace with actual image URL or local file path\n",
        "predict_image(image_path, model)\n"
      ],
      "metadata": {
        "id": "3o2nYpg1Q8SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rgb 200"
      ],
      "metadata": {
        "id": "0D0FHWAdS2x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# Define dataset paths\n",
        "train_dir = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/train\"\n",
        "val_dir = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/valid\"\n",
        "\n",
        "# Define label mapping\n",
        "label_mapping = {\"FRESH\": 0, \"SPOILED\": 1}\n",
        "\n",
        "# Function to extract label from filename\n",
        "def get_label_from_filename(filename):\n",
        "    if \"FRESH\" in filename.upper():\n",
        "        return label_mapping[\"FRESH\"]\n",
        "    elif \"SPOILED\" in filename.upper():\n",
        "        return label_mapping[\"SPOILED\"]\n",
        "    else:\n",
        "        return None  # Ignore unexpected filenames\n",
        "\n",
        "# Function to load images and labels\n",
        "def load_images(folder, img_size=(200, 200)):  # Changed image size to 200x200\n",
        "    images, labels = [], []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, file)\n",
        "\n",
        "        label = get_label_from_filename(file)\n",
        "        if label is not None:\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "                img = cv2.resize(img, img_size)  # Resize image to 200x200\n",
        "                img = img / 255.0  # Normalize\n",
        "\n",
        "                images.append(img)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load train and validation data\n",
        "X_train, y_train = load_images(train_dir, img_size=(200, 200))\n",
        "X_val, y_val = load_images(val_dir, img_size=(200, 200))\n",
        "\n",
        "print(f\"Loaded {len(X_train)} training images and {len(X_val)} validation images\")\n",
        "\n",
        "# Define CNN Model with 3 Convolutional Layers\n",
        "def build_cnn(input_shape):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')  # Binary Classification (Fresh/Spoiled)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = build_cnn((200, 200, 3))  # Changed input shape to 200x200\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the CNN...\")\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "# Function to load and preprocess image from a URL or local path\n",
        "def preprocess_image(path, target_size=(200, 200)):  # Changed target size to 200x200\n",
        "    \"\"\"\n",
        "    Loads and preprocesses an image from a URL or local file path.\n",
        "\n",
        "    Args:\n",
        "        path (str): The URL or local file path of the image.\n",
        "        target_size (tuple): The desired size of the image (width, height).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The preprocessed image as a NumPy array.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt to open the path as a URL\n",
        "        response = requests.get(path)\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    except (requests.exceptions.MissingSchema, requests.exceptions.InvalidSchema):\n",
        "        # If it's not a valid URL, assume it's a local file path\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "    img = img.resize(target_size)  # Resize image to 200x200\n",
        "    img_array = np.array(img) / 255.0  # Normalize\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img_array\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_image(path, model):\n",
        "    img_array = preprocess_image(path)\n",
        "    prediction = model.predict(img_array)[0][0]  # Get single prediction\n",
        "    if prediction > 0.5:\n",
        "        print(\"Prediction: ❌ Spoiled Meat\")\n",
        "    else:\n",
        "        print(\"Prediction: ✅ Fresh Meat\")\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/train/FRESH-1-_JPG.rf.65663691924ca0aede3884b863267c98.jpg\"  # Replace with actual image path\n",
        "predict_image(image_path, model)\n"
      ],
      "metadata": {
        "id": "HBWdJt9-SnIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "greyscale 200"
      ],
      "metadata": {
        "id": "lgCo5BR8TQOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# Define dataset paths\n",
        "train_dir = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/train\"\n",
        "val_dir = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/valid\"\n",
        "\n",
        "# Define label mapping\n",
        "label_mapping = {\"FRESH\": 0, \"SPOILED\": 1}\n",
        "\n",
        "# Function to extract label from filename\n",
        "def get_label_from_filename(filename):\n",
        "    if \"FRESH\" in filename.upper():\n",
        "        return label_mapping[\"FRESH\"]\n",
        "    elif \"SPOILED\" in filename.upper():\n",
        "        return label_mapping[\"SPOILED\"]\n",
        "    else:\n",
        "        return None  # Ignore unexpected filenames\n",
        "\n",
        "# Function to load images and labels (convert to grayscale)\n",
        "def load_images(folder, img_size=(200, 200)):\n",
        "    images, labels = [], []\n",
        "\n",
        "    for file in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, file)\n",
        "\n",
        "        label = get_label_from_filename(file)\n",
        "        if label is not None:\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, img_size)  # Resize image to 200x200\n",
        "                img = img / 255.0  # Normalize\n",
        "\n",
        "                images.append(img)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(images).reshape(-1, 200, 200, 1), np.array(labels)  # Reshape for CNN\n",
        "\n",
        "# Load train and validation data\n",
        "X_train, y_train = load_images(train_dir, img_size=(200, 200))\n",
        "X_val, y_val = load_images(val_dir, img_size=(200, 200))\n",
        "\n",
        "print(f\"Loaded {len(X_train)} training images and {len(X_val)} validation images\")\n",
        "\n",
        "# Define CNN Model with 3 Convolutional Layers\n",
        "def build_cnn(input_shape):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')  # Binary Classification (Fresh/Spoiled)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = build_cnn((200, 200, 1))  # Changed input shape to (200, 200, 1) for grayscale\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the CNN...\")\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "# Function to load and preprocess image from a URL or local path (convert to grayscale)\n",
        "def preprocess_image(path, target_size=(200, 200)):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses an image from a URL or local file path.\n",
        "\n",
        "    Args:\n",
        "        path (str): The URL or local file path of the image.\n",
        "        target_size (tuple): The desired size of the image (width, height).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The preprocessed image as a NumPy array.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt to open the path as a URL\n",
        "        response = requests.get(path)\n",
        "        img = Image.open(BytesIO(response.content)).convert(\"L\")  # Convert to grayscale\n",
        "    except (requests.exceptions.MissingSchema, requests.exceptions.InvalidSchema):\n",
        "        # If it's not a valid URL, assume it's a local file path\n",
        "        img = Image.open(path).convert(\"L\")  # Convert to grayscale\n",
        "\n",
        "    img = img.resize(target_size)  # Resize image to 200x200\n",
        "    img_array = np.array(img) / 255.0  # Normalize\n",
        "    img_array = np.expand_dims(img_array, axis=(0, -1))  # Add batch and channel dimensions\n",
        "    return img_array\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_image(path, model):\n",
        "    img_array = preprocess_image(path)\n",
        "    prediction = model.predict(img_array)[0][0]  # Get single prediction\n",
        "    if prediction > 0.5:\n",
        "        print(\"Prediction: ❌ Spoiled Meat\")\n",
        "    else:\n",
        "        print(\"Prediction: ✅ Fresh Meat\")\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/drive/MyDrive/Meat Freshness.v1-new-dataset.multiclass/train/FRESH-1-_JPG.rf.65663691924ca0aede3884b863267c98.jpg\"\n",
        "predict_image(image_path, model)\n"
      ],
      "metadata": {
        "id": "bB_C9XCvTVh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Assuming you have:\n",
        "# y_test → true labels\n",
        "# y_pred → predicted labels\n",
        "\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))  # use 'binary' for 2 classes\n",
        "print(\"Recall   :\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score :\", f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "T4AkYlxJt3PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from statsmodels.stats.weightstats import ztest\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "# Simulate \"freshness score\" feature for 2 classes\n",
        "fresh_scores = np.random.normal(loc=85, scale=5, size=50)   # Higher score = more fresh\n",
        "spoiled_scores = np.random.normal(loc=60, scale=7, size=50) # Lower score = more spoiled\n",
        "\n",
        "# Combine into DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'FreshnessScore': np.concatenate((fresh_scores, spoiled_scores)),\n",
        "    'Label': ['Fresh']*50 + ['Spoiled']*50\n",
        "})\n"
      ],
      "metadata": {
        "id": "BoNj7exknsn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "r6_FGgW3uHWf",
        "outputId": "7432c7d5-6ba5-49b5-cb22-a33f967ec8fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e1b8ab2e2d75>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z_stat, p_val = ztest(spoiled_scores, value=65)\n",
        "print(f\"Z-Test: z = {z_stat:.2f}, p = {p_val:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCJC5D1jnyTN",
        "outputId": "ebab8ec4-a24c-4863-9439-6e8a40978122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z-Test: z = -5.08, p = 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_stat, p_val = stats.ttest_ind(fresh_scores, spoiled_scores)\n",
        "print(f\"T-Test: t = {t_stat:.2f}, p = {p_val:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZEIcr7Nn15d",
        "outputId": "295c0a84-93f8-47e6-c6c9-4c457a6acb92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-Test: t = 22.82, p = 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_stat = np.var(fresh_scores, ddof=1) / np.var(spoiled_scores, ddof=1)\n",
        "df1, df2 = len(fresh_scores) - 1, len(spoiled_scores) - 1\n",
        "p_val = 1 - stats.f.cdf(f_stat, df1, df2)\n",
        "print(f\"F-Test: F = {f_stat:.2f}, p = {p_val:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMrDWUGdn7NB",
        "outputId": "e2c84d64-498d-4753-c306-ee17f5b944b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-Test: F = 0.78, p = 0.8018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate 3 groups\n",
        "group1 = np.random.normal(85, 5, 30)  # Fresh\n",
        "group2 = np.random.normal(70, 5, 30)  # Slightly Spoiled\n",
        "group3 = np.random.normal(55, 6, 30)  # Heavily Spoiled\n",
        "\n",
        "f_stat, p_val = stats.f_oneway(group1, group2, group3)\n",
        "print(f\"ANOVA Test: F = {f_stat:.2f}, p = {p_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "CAyQmklIn_E-",
        "outputId": "1e9a7fb7-ad10-467d-ecb3-6de0583d90df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANOVA Test: F = 255.88, p = 0.0000\n"
          ]
        }
      ]
    }
  ]
}